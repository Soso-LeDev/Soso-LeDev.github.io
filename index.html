<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Détection Visages - Webcam Live</title>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
    <script src="https://cdn.jsdelivr.net/npm/face-api.js"></script>
    <style>
        body { margin: 0; font-family: Arial; text-align: center; background: #f0f0f0; }
        #video { width: 640px; height: 480px; border: 2px solid #ccc; }
        #status { margin: 10px; font-size: 18px; color: #333; }
    </style>
</head>
<body>
    <h1>Détection de Visages en Live</h1>
    <p id="status">Chargement des modèles... Autorise l'accès à la caméra.</p>
    <video id="video" autoplay muted></video>
    <br>
    <button onclick="stopDetection()">Arrêter</button>

    <script>
        const video = document.getElementById('video');
        const status = document.getElementById('status');

        // Charger les modèles face-api.js
        Promise.all([
            faceapi.nets.tinyFaceDetector.loadFromUri('https://justadudewhohacks.face-api.net/models'),
            faceapi.nets.faceLandmark68Net.loadFromUri('https://justadudewhohacks.face-api.net/models'),
            faceapi.nets.faceRecognitionNet.loadFromUri('https://justadudewhohacks.face-api.net/models')
        ]).then(startVideo);

        function startVideo() {
            status.textContent = 'Modèles chargés ! Accès caméra...';
            navigator.mediaDevices.getUserMedia({ video: {} })
                .then(stream => {
                    video.srcObject = stream;
                    video.addEventListener('play', () => {
                        const canvas = faceapi.createCanvasFromMedia(video);
                        document.body.append(canvas);
                        const displaySize = { width: video.width, height: video.height };
                        faceapi.matchDimensions(canvas, displaySize);
                        setInterval(async () => {
                            const detections = await faceapi.detectAllFaces(video, new faceapi.TinyFaceDetectorOptions())
                                .withFaceLandmarks().withFaceDescriptors();
                            const resizedDetections = faceapi.resizeResults(detections, displaySize);
                            canvas.getContext('2d').clearRect(0, 0, canvas.width, canvas.height);
                            faceapi.draw.drawDetections(canvas, resizedDetections);
                            status.textContent = `Visages détectés : ${detections.length}`;
                        }, 100);  // Détection toutes les 100ms (ajustable pour perf)
                    });
                })
                .catch(err => {
                    status.textContent = `Erreur caméra : ${err.message}`;
                });
        }

        function stopDetection() {
            video.srcObject.getTracks().forEach(track => track.stop());
            status.textContent = 'Détection arrêtée.';
        }
    </script>
</body>
</html>
