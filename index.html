<!DOCTYPE html>
<html lang="fr">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Détection visage — fichier unique</title>
  <style>
    html,body { height:100%; margin:0; background:#000; color:#fff; font-family:Inter,Arial,sans-serif; }
    video, canvas {
      position: absolute;
      top:0; left:0;
      width:100vw; height:100vh;
      object-fit:cover;
    }
    #status {
      position: fixed;
      bottom: 12px;
      left: 50%;
      transform: translateX(-50%);
      background: rgba(0,0,0,0.6);
      padding: 8px 12px;
      border-radius: 8px;
      font-size: 14px;
      z-index: 20;
    }
  </style>

  <!-- 1) Charger TFJS et BlazeFace via CDN (ordre important) -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.21.0/dist/tf.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/blazeface@0.0.7/dist/blazeface.min.js"></script>
</head>
<body>
  <video id="video" autoplay muted playsinline></video>
  <canvas id="overlay"></canvas>
  <div id="status">Initialisation…</div>

  <!-- Script principal (tout dans le même fichier) -->
  <script>
    const video = document.getElementById('video');
    const canvas = document.getElementById('overlay');
    const ctx = canvas.getContext('2d');
    const statusEl = document.getElementById('status');
    let model = null;

    // Utilitaire : attend qu'une variable globale existe (polling) avec timeout
    function waitForGlobal(name, ms = 5000, interval = 100) {
      return new Promise((resolve, reject) => {
        if (window[name]) return resolve(window[name]);
        const t = setInterval(() => {
          if (window[name]) {
            clearInterval(t);
            resolve(window[name]);
          }
        }, interval);
        setTimeout(() => {
          clearInterval(t);
          reject(new Error(name + ' introuvable après ' + ms + 'ms'));
        }, ms);
      });
    }

    async function setupCamera() {
      try {
        statusEl.textContent = "Demande d'accès à la caméra…";
        const stream = await navigator.mediaDevices.getUserMedia({ video: { facingMode: "user" }, audio: false });
        video.srcObject = stream;
        return new Promise(resolve => {
          video.onloadedmetadata = () => {
            video.play();
            resizeCanvas();
            resolve();
          };
        });
      } catch (err) {
        statusEl.textContent = "Erreur accès caméra : " + (err.message || err);
        throw err;
      }
    }

    function resizeCanvas() {
      // On met le canvas en taille réelle vidéo (pixels)
      const w = video.videoWidth || window.innerWidth;
      const h = video.videoHeight || window.innerHeight;
      canvas.width = w;
      canvas.height = h;
      // CSS pour couvrir
      canvas.style.width = "100vw";
      canvas.style.height = "100vh";
    }
    window.addEventListener('resize', resizeCanvas);

    async function loadModelWithGuard() {
      try {
        statusEl.textContent = "Attente du chargement de BlazeFace…";
        // attend que la lib blazeface soit définie globalement par le CDN
        await waitForGlobal('blazeface', 7000).catch(err => {
          // échec du polling : on laisse l'erreur remonter
          throw err;
        });
        // maintenant charger le modèle
        statusEl.textContent = "Chargement du modèle BlazeFace…";
        model = await blazeface.load();
        statusEl.textContent = "Modèle chargé ✅";
      } catch (err) {
        statusEl.textContent = "Impossible de charger BlazeFace : " + (err.message || err);
        console.error(err);
        throw err;
      }
    }

    async function detectLoop() {
      if (!model || video.readyState < 2) {
        requestAnimationFrame(detectLoop);
        return;
      }
      resizeCanvas();

      try {
        // false -> on veut des coordonnées JS (pas des tensors)
        const preds = await model.estimateFaces(video, false);
        ctx.clearRect(0, 0, canvas.width, canvas.height);

        if (preds && preds.length > 0) {
          statusEl.textContent = `Visage détecté ✅ (${preds.length})`;
          for (let p of preds) {
            const [x1,y1] = p.topLeft;
            const [x2,y2] = p.bottomRight;
            const w = x2 - x1;
            const h = y2 - y1;
            ctx.lineWidth = Math.max(2, Math.round(canvas.width / 300));
            ctx.strokeStyle = "#00FF00";
            ctx.strokeRect(x1, y1, w, h);

            // landmarks
            if (p.landmarks && p.landmarks.length) {
              ctx.fillStyle = "#00FF00";
              for (const lm of p.landmarks) {
                ctx.beginPath();
                ctx.arc(lm[0], lm[1], Math.max(1, canvas.width/500), 0, Math.PI*2);
                ctx.fill();
              }
            }
          }
        } else {
          statusEl.textContent = "Aucun visage détecté ❌";
        }
      } catch (err) {
        // erreurs possibles si modèle déchargé ou input invalide
        console.error("Erreur détection :", err);
        statusEl.textContent = "Erreur détection : " + (err.message || err);
      }

      requestAnimationFrame(detectLoop);
    }

    (async function main() {
      try {
        await setupCamera();

        // Important : si tu as encore l'erreur "blazeface is not defined", c'est que
        // le CDN n'a pas chargé ou le fichier renvoie 404. On essaie ici de charger proprement.
        await loadModelWithGuard();

        // Lancer la boucle de détection
        detectLoop();
      } catch (err) {
        console.error("Erreur initialisation :", err);
        // message déjà mis par les catch internes
      }
    })();
  </script>
</body>
</html>
