<!doctype html>
<html lang="fr">
<head>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<title>Face Mesh — démo</title>
<style>
  html,body{height:100%;margin:0;background:#000;color:#fff;font-family:Arial,Helvetica,sans-serif}
  #container{position:relative;width:100vw;height:100vh;overflow:hidden}
  video{position:absolute;top:0;left:0;width:100%;height:100%;object-fit:cover}
  canvas{position:absolute;top:0;left:0;pointer-events:none}
  #hud{position:fixed;left:12px;top:12px;background:rgba(0,0,0,0.6);padding:8px;border-radius:8px;font-size:13px;z-index:50}
  #controls{position:fixed;right:12px;top:12px;background:rgba(0,0,0,0.6);padding:8px;border-radius:8px;font-size:13px;z-index:50}
  .corner{position:fixed;z-index:40}
</style>

<!-- TFJS + face-landmarks-detection (UMD bundles via jsdelivr) -->
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.18.0/dist/tf.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/face-landmarks-detection@1.0.6/dist/face-landmarks-detection.min.js"></script>
</head>
<body>
  <div id="container">
    <video id="video" autoplay muted playsinline></video>
    <canvas id="overlay"></canvas>
  </div>

  <div id="hud">Initialisation…</div>

  <div id="controls">
    <div>Mesh density: <input id="density" type="range" min="1" max="12" value="5"></div>
    <div><label><input id="showDots" type="checkbox" checked> Dots</label></div>
    <div><label><input id="showAnnotations" type="checkbox" checked> Annotations outlines</label></div>
    <div style="margin-top:6px;font-size:12px;color:#ccc">Tester sur localhost / HTTPS</div>
  </div>

<script>
(async () => {
  const video = document.getElementById('video');
  const canvas = document.getElementById('overlay');
  const ctx = canvas.getContext('2d');
  const hud = document.getElementById('hud');
  const densityInput = document.getElementById('density');
  const showDotsCheckbox = document.getElementById('showDots');
  const showAnnotationsCheckbox = document.getElementById('showAnnotations');

  // accès caméra
  try {
    const stream = await navigator.mediaDevices.getUserMedia({video:{facingMode:"user"}, audio:false});
    video.srcObject = stream;
    await new Promise(resolve => video.onloadedmetadata = resolve);
    video.play();
  } catch (e) {
    hud.textContent = "Erreur accès caméra: " + (e.message || e);
    console.error(e);
    return;
  }

  // aligne le canvas sur la vidéo (taille pixel)
  function syncCanvas() {
    const vw = video.videoWidth || video.clientWidth;
    const vh = video.videoHeight || video.clientHeight;
    canvas.width = vw;
    canvas.height = vh;
    canvas.style.width = video.clientWidth + 'px';
    canvas.style.height = video.clientHeight + 'px';
    const rect = video.getBoundingClientRect();
    canvas.style.left = rect.left + 'px';
    canvas.style.top = rect.top + 'px';
  }
  window.addEventListener('resize', syncCanvas);
  syncCanvas();

  // attendre que faceLandmarksDetection soit disponible dans window
  function waitForGlobal(name, ms = 7000, interval = 100) {
    return new Promise((resolve, reject) => {
      if (window[name]) return resolve(window[name]);
      const t = setInterval(() => { if (window[name]) { clearInterval(t); resolve(window[name]); } }, interval);
      setTimeout(() => { clearInterval(t); reject(new Error(name + " introuvable")); }, ms);
    });
  }

  // charger le modèle MediaPipeFaceMesh
  try {
    await waitForGlobal('faceLandmarksDetection', 7000);
    hud.textContent = "Chargement du modèle…";
    // SupportedPackages.mediapipeFacemesh utilise MediaPipe backend (plus fiable pour 468 points)
    const model = await faceLandmarksDetection.load(faceLandmarksDetection.SupportedPackages.mediapipeFacemesh, { maxFaces: 1 });
    hud.textContent = "Modèle chargé ✅";
    console.log("Model loaded:", model);

    // utilitaire dessin
    function drawLine(p1, p2, width=1, color='rgba(0,255,255,0.6)') {
      ctx.strokeStyle = color;
      ctx.lineWidth = width;
      ctx.beginPath();
      ctx.moveTo(p1[0], p1[1]);
      ctx.lineTo(p2[0], p2[1]);
      ctx.stroke();
    }
    function drawCircle(p, r=2, color='rgba(255,200,0,0.9)') {
      ctx.fillStyle = color;
      ctx.beginPath();
      ctx.arc(p[0], p[1], r, 0, Math.PI*2);
      ctx.fill();
    }
    function drawPath(points, close=false, width=1, color='rgba(0,255,0,0.6)') {
      if(!points || points.length===0) return;
      ctx.strokeStyle = color;
      ctx.lineWidth = width;
      ctx.beginPath();
      ctx.moveTo(points[0][0], points[0][1]);
      for (let i=1;i<points.length;i++) ctx.lineTo(points[i][0], points[i][1]);
      if (close) ctx.closePath();
      ctx.stroke();
    }

    // boucle de detection
    async function loop() {
      syncCanvas();
      if (video.readyState < 2) { requestAnimationFrame(loop); return; }

      try {
        // estimateFaces API: retourne scaledMesh (468 pts) + annotations object
        const predictions = await model.estimateFaces({input: video, returnTensors: false, flipHorizontal: false});
        ctx.clearRect(0,0,canvas.width,canvas.height);

        if (!predictions || predictions.length === 0) {
          hud.textContent = "Aucun visage détecté ❌";
          requestAnimationFrame(loop);
          return;
        }

        const pred = predictions[0];
        const keypoints = pred.scaledMesh || pred.mesh || pred.keypoints || [];
        const annotations = pred.annotations || {};

        // corner brackets (HUD)
        const pad = 18;
        const cornerColor = 'rgba(200,255,0,0.9)';
        const w = canvas.width, h = canvas.height;
        const len = 28;
        ctx.strokeStyle = cornerColor; ctx.lineWidth = 4;
        ctx.beginPath();
        // top-left
        ctx.moveTo(pad, pad + len); ctx.lineTo(pad, pad); ctx.lineTo(pad + len, pad);
        // top-right
        ctx.moveTo(w - pad - len, pad); ctx.lineTo(w - pad, pad); ctx.lineTo(w - pad, pad + len);
        // bottom-left
        ctx.moveTo(pad, h - pad - len); ctx.lineTo(pad, h - pad); ctx.lineTo(pad + len, h - pad);
        // bottom-right
        ctx.moveTo(w - pad - len, h - pad); ctx.lineTo(w - pad, h - pad); ctx.lineTo(w - pad, h - pad - len);
        ctx.stroke();

        // draw annotations outlines (like eyes/nose/mouth contours)
        if (showAnnotationsCheckbox.checked && annotations) {
          ctx.lineWidth = Math.max(1, canvas.width/600);
          for (const name in annotations) {
            const pts = annotations[name].map(p => [p[0], p[1]]);
            const closed = (name === 'lipsUpperOuter' || name === 'lipsLowerOuter' || name === 'faceOval' || name === 'lipsUpperInner' || name === 'lipsLowerInner');
            drawPath(pts, closed, Math.max(1, canvas.width/800), 'rgba(0,200,255,0.85)');
          }
        }

        // draw mesh "criss-cross" dense: for each point connect to next N neighbors (density slider)
        const density = Math.max(1, parseInt(densityInput.value, 10));
        ctx.lineWidth = Math.max(0.5, canvas.width/1500);
        ctx.strokeStyle = 'rgba(255,255,255,0.08)'; // subtle
        const L = keypoints.length;
        // sample to reduce cost if too many points (but L ~468 ok)
        for (let i = 0; i < L; i++) {
          const p = keypoints[i];
          const px = p[0], py = p[1];
          // connect to next density points (wrap)
          for (let d = 1; d <= density; d++) {
            const j = (i + d) % L;
            const q = keypoints[j];
            ctx.beginPath();
            ctx.moveTo(px, py);
            ctx.lineTo(q[0], q[1]);
            ctx.stroke();
          }
        }

        // draw stronger connections for facial structure by linking every Kth point
        ctx.strokeStyle = 'rgba(0,255,200,0.12)';
        ctx.lineWidth = Math.max(0.4, canvas.width/1800);
        const step = Math.max(4, Math.floor(480 / (density*4))); // heuristic
        for (let i = 0; i < L; i += step) {
          for (let j = 1; j <= 3; j++) {
            const idx = (i + j*step) % L;
            ctx.beginPath(); ctx.moveTo(keypoints[i][0], keypoints[i][1]); ctx.lineTo(keypoints[idx][0], keypoints[idx][1]); ctx.stroke();
          }
        }

        // draw dots optionally (landmarks)
        if (showDotsCheckbox.checked) {
          const r = Math.max(0.6, canvas.width/700);
          for (let i = 0; i < L; i += Math.max(1, Math.floor(10 - density))) {
            const p = keypoints[i];
            ctx.fillStyle = "rgba(255,180,60,0.9)";
            ctx.beginPath(); ctx.arc(p[0], p[1], r, 0, Math.PI*2); ctx.fill();
          }
        }

        // center cross and distance between eyes (use annotations if available)
        let leftEyeCenter = null, rightEyeCenter = null;
        if (annotations.leftEyeUpper0 && annotations.rightEyeUpper0) {
          const left = annotations.leftEyeUpper0.concat(annotations.leftEyeLower0 || []);
          const right = annotations.rightEyeUpper0.concat(annotations.rightEyeLower0 || []);
          leftEyeCenter = left.reduce((acc,p)=>[acc[0]+p[0], acc[1]+p[1]], [0,0]).map(v => v/left.length);
          rightEyeCenter = right.reduce((acc,p)=>[acc[0]+p[0], acc[1]+p[1]], [0,0]).map(v => v/right.length);
        } else if (keypoints && keypoints.length >= 10) {
          // fallback: use points ~33 and ~263 (typical facemesh indices for eyes)
          leftEyeCenter = keypoints[33] || null;
          rightEyeCenter = keypoints[263] || null;
        }

        if (leftEyeCenter && rightEyeCenter) {
          // draw line between eye centers
          drawLine(leftEyeCenter, rightEyeCenter, Math.max(1, canvas.width/500), 'rgba(0,255,255,0.9)');
          // draw central square
          const midX = (leftEyeCenter[0] + rightEyeCenter[0]) / 2;
          const midY = (leftEyeCenter[1] + rightEyeCenter[1]) / 2;
          const box = Math.max(8, canvas.width/160);
          ctx.fillStyle = 'rgba(255,0,120,0.95)';
          ctx.fillRect(midX - box/2, midY - box/2, box, box);
          // show distance px
          const dx = rightEyeCenter[0] - leftEyeCenter[0];
          const dy = rightEyeCenter[1] - leftEyeCenter[1];
          const distPx = Math.round(Math.hypot(dx,dy));
          hud.textContent = `Visage détecté ✅ — dist yeux: ${distPx}px — points: ${L}`;
        } else {
          hud.textContent = `Visage détecté ✅ — points: ${L}`;
        }

      } catch (err) {
        console.error("Erreur boucle:", err);
        hud.textContent = "Erreur de détection: " + (err.message || err);
      }

      requestAnimationFrame(loop);
    }

    loop();

  } catch (err) {
    console.error("Erreur init modèle:", err);
    hud.textContent = "Erreur chargement modèle : " + (err.message || err);
  }

})();
</script>
</body>
</html>
